{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Session 4 Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravimashru/END-assignments/blob/main/Assignment_4_Sentiment_Analysis_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_RfWV1HEpGL"
      },
      "source": [
        "# END - Assignment 4\n",
        "\n",
        "In this assignment, [this notebook](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb) was used as a starting point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvJtZK9Sjg4n"
      },
      "source": [
        "---\n",
        "### Assignment\n",
        "---\n",
        "\n",
        "Change this code in such a way that:\n",
        "\n",
        "1. it has 3 LSTM layers\n",
        "2. it has used a for loop to do so in the forward function\n",
        "3. the dropout value used is 0.2\n",
        "4. trained on the text that is reversed (for example \"my name is Rohan\" becomes \"Rohan is name my\"\n",
        "5. achieves 87% or more accuracy\n",
        "6. once done, share the Github link as well (after training on Google Colab, move the file to GitHub).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj7AuWHQiZiN",
        "outputId": "dd23670c-da90-40a9-98cd-a2ec2a916092"
      },
      "source": [
        "print(torch. __version__)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_upwp4viw8d"
      },
      "source": [
        "# Colab System Specs\n",
        "# https://colab.research.google.com/drive/151805XTDg--dgHb3-AXJCpnWaqRhop_2\n",
        "\n",
        "# CPU info\n",
        "# !cat /proc/cpuinfo\n",
        "\n",
        "# Memory Info\n",
        "# !cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wPG98IZrmMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26fb2adc-eade-45b6-9048-998a51288bf3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Nov 19 12:02:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHoOcChSrgP2"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGQEqVknrgP6"
      },
      "source": [
        "We then load the IMDb dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVPGjYAsrgP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f2d21a-dbd0-41f8-d478-354995a38f11"
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 10.8MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqfSnL64rgP9"
      },
      "source": [
        "Then create the validation set from our training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpU0yqvkrgP9"
      },
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2bqRGf_rgQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656a73cc-a877-42d8-9e2a-b565fafbc2dc"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.200d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                          \n",
            "100%|█████████▉| 399067/400000 [00:40<00:00, 10002.23it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuS1wxOqrgQD"
      },
      "source": [
        "As before, we create the iterators, placing the tensors on the GPU if one is available.\n",
        "\n",
        "Another thing for packed padded sequences all of the tensors within a batch need to be sorted by their lengths. This is handled in the iterator by setting `sort_within_batch = True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94yATV6nrgQE"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = False,\n",
        "    device = device, )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MfJ5ZUkftlK"
      },
      "source": [
        "## Multiple LSTM Layers\n",
        "The original notebook had 2 stacked LSTMs using the `num_layers` property of `torch.nn.LSTM`.\n",
        "\n",
        "In this assignment, an instance of `torch.nn.ModuleList` was used to hold 3 individual instances of `torch.nn.LSTM` and a `for` loop was used to pass data between the LSTM cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi_0NS3argQG"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "\n",
        "        self.rnns = nn.ModuleList([\n",
        "                      nn.LSTM(embedding_dim, \n",
        "                           hidden_dim,\n",
        "                           bidirectional=bidirectional),\n",
        "\n",
        "                      nn.LSTM(hidden_dim, \n",
        "                           hidden_dim,\n",
        "                           bidirectional=bidirectional),\n",
        "\n",
        "                      nn.LSTM(hidden_dim, \n",
        "                           hidden_dim,\n",
        "                           bidirectional=bidirectional)\n",
        "        ])\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.inplace_dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted=False)\n",
        "        \n",
        "        # packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        for i, rnn in enumerate(self.rnns):\n",
        "          packed_embedded, (hidden, cell) = rnn(packed_embedded)\n",
        "          self.inplace_dropout(packed_embedded.data)\n",
        "        \n",
        "        # hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        hidden = self.dropout(hidden)\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e78dPCq4f16_"
      },
      "source": [
        "## Reduced dropout\n",
        "The value of the dropout had to be reduced from a value of 0.5 in the original notebook to 0.2.\n",
        "\n",
        "## Training on reverse text\n",
        "To understand how bi-directional LSTMs work, the `bidirectional` property was set to `False` and the input to the model was manually reversed using the `torch.flip` method.\n",
        "\n",
        "This resulted in an accuracy as high as that obtained when using `bidirectional=True` which makes the behavior of that property clear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIZ4JqwDrgQK"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "BIDIRECTIONAL = False\n",
        "DROPOUT = 0.2\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM,\n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vtlQ2iVrgQM"
      },
      "source": [
        "We'll print out the number of parameters in our model. \n",
        "\n",
        "Notice how we have almost twice as many parameters as before!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJDFwOGErgQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a88c08e-8b6f-445a-bab2-e2deece4691b"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 6,522,321 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7-KZI8VrgQP"
      },
      "source": [
        "The final addition is copying the pre-trained word embeddings we loaded earlier into the `embedding` layer of our model.\n",
        "\n",
        "We retrieve the embeddings from the field's vocab, and check they're the correct size, _**[vocab size, embedding dim]**_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSyVu7r6rgQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7c7cfc-1b12-444c-9c34-ef681ce95366"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCobWFqxrgQS"
      },
      "source": [
        "We then replace the initial weights of the `embedding` layer with the pre-trained embeddings.\n",
        "\n",
        "**Note**: this should always be done on the `weight.data` and not the `weight`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bStwFi3zrgQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edf387e-cf47-42b5-c3c3-a921cbd6e298"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ..., -1.8542,  0.4022,  0.4238],\n",
              "        [ 0.2078,  1.1879, -0.7320,  ...,  1.3663, -0.4598,  0.6668],\n",
              "        [-0.0715,  0.0935,  0.0237,  ...,  0.3362,  0.0306,  0.2558],\n",
              "        ...,\n",
              "        [ 0.3127,  0.4397,  0.0985,  ..., -0.0854, -0.0217, -0.1378],\n",
              "        [-0.4698, -0.2754, -0.1802,  ...,  0.1676, -0.1875,  0.3729],\n",
              "        [ 0.9528, -0.1623, -0.2516,  ..., -0.4873,  0.1463,  0.2355]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWvXlDRUrgQU"
      },
      "source": [
        "**Note**: like initializing the embeddings, this should be done on the `weight.data` and not the `weight`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-KnRmkgrgQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e29ece7-8add-4e1a-ad6c-87db22d72565"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0715,  0.0935,  0.0237,  ...,  0.3362,  0.0306,  0.2558],\n",
            "        ...,\n",
            "        [ 0.3127,  0.4397,  0.0985,  ..., -0.0854, -0.0217, -0.1378],\n",
            "        [-0.4698, -0.2754, -0.1802,  ...,  0.1676, -0.1875,  0.3729],\n",
            "        [ 0.9528, -0.1623, -0.2516,  ..., -0.4873,  0.1463,  0.2355]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peYcNeAargQX"
      },
      "source": [
        "We can now see the first two rows of the embedding weights matrix have been set to zeros. As we passed the index of the pad token to the `padding_idx` of the embedding layer it will remain zeros throughout training, however the `<unk>` token embedding will be learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5T3psmBrgQY"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHGdoYwvrgQZ"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzh5O5dErgQb"
      },
      "source": [
        "The rest of the steps for training the model are unchanged.\n",
        "\n",
        "We define the criterion and place the model and criterion on the GPU (if available)..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTPYreNargQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd32995b-52f0-47df-e125-b6bcf5cd0c2b"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399067/400000 [01:00<00:00, 10002.23it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbeu4sm1rgQe"
      },
      "source": [
        "We implement the function to calculate accuracy..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O0RA5RdrgQe"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TI8rPCIrgQg"
      },
      "source": [
        "We define a function for training our model. \n",
        "\n",
        "As we have set `include_lengths = True`, our `batch.text` is now a tuple with the first element being the numericalized tensor and the second element being the actual lengths of each sequence. We separate these into their own variables, `text` and `text_lengths`, before passing them to the model.\n",
        "\n",
        "**Note**: as we are now using dropout, we must remember to use `model.train()` to ensure the dropout is \"turned on\" while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F39p24-YrgQg"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        # Reverse the sentences\n",
        "        # dim 0 => sentence\n",
        "        # dim 1 => batch\n",
        "        reverse_text = torch.flip(text, [0])\n",
        "\n",
        "        combined_text = torch.cat([text, reverse_text], axis=1)\n",
        "        combined_labels = torch.cat([batch.label, batch.label])\n",
        "        combined_lengths = torch.cat([text_lengths, text_lengths])\n",
        "        \n",
        "        predictions = model(combined_text, combined_lengths.cpu()).squeeze()\n",
        "\n",
        "        loss = criterion(predictions, combined_labels)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, combined_labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtflNhfcrgQi"
      },
      "source": [
        "Then we define a function for testing our model, again remembering to separate `batch.text`.\n",
        "\n",
        "**Note**: as we are now using dropout, we must remember to use `model.eval()` to ensure the dropout is \"turned off\" while evaluating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwE9wYGWrgQj"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = model(text, text_lengths.cpu()).squeeze()\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW9rQC5trgQl"
      },
      "source": [
        "And also create a nice function to tell us how long our epochs are taking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOp_mcS_rgQl"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktlyp9jcrgQn"
      },
      "source": [
        "Finally, we train our model..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqCtLD8grgQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ecf73f-839e-4e7a-fb45-ec20abb9987a"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        print('Saving best model weights...')\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving best model weights...\n",
            "Epoch: 01 | Epoch Time: 3m 50s\n",
            "\tTrain Loss: 0.679 | Train Acc: 55.14%\n",
            "\t Val. Loss: 0.584 |  Val. Acc: 69.85%\n",
            "Epoch: 02 | Epoch Time: 3m 47s\n",
            "\tTrain Loss: 0.620 | Train Acc: 62.51%\n",
            "\t Val. Loss: 0.631 |  Val. Acc: 63.81%\n",
            "Saving best model weights...\n",
            "Epoch: 03 | Epoch Time: 3m 47s\n",
            "\tTrain Loss: 0.599 | Train Acc: 64.12%\n",
            "\t Val. Loss: 0.479 |  Val. Acc: 78.67%\n",
            "Saving best model weights...\n",
            "Epoch: 04 | Epoch Time: 3m 47s\n",
            "\tTrain Loss: 0.543 | Train Acc: 68.04%\n",
            "\t Val. Loss: 0.463 |  Val. Acc: 79.64%\n",
            "Saving best model weights...\n",
            "Epoch: 05 | Epoch Time: 3m 47s\n",
            "\tTrain Loss: 0.595 | Train Acc: 63.90%\n",
            "\t Val. Loss: 0.402 |  Val. Acc: 83.14%\n",
            "Saving best model weights...\n",
            "Epoch: 06 | Epoch Time: 3m 48s\n",
            "\tTrain Loss: 0.498 | Train Acc: 70.21%\n",
            "\t Val. Loss: 0.401 |  Val. Acc: 84.56%\n",
            "Saving best model weights...\n",
            "Epoch: 07 | Epoch Time: 3m 46s\n",
            "\tTrain Loss: 0.460 | Train Acc: 72.20%\n",
            "\t Val. Loss: 0.322 |  Val. Acc: 86.53%\n",
            "Epoch: 08 | Epoch Time: 3m 48s\n",
            "\tTrain Loss: 0.432 | Train Acc: 73.34%\n",
            "\t Val. Loss: 0.365 |  Val. Acc: 86.65%\n",
            "Saving best model weights...\n",
            "Epoch: 09 | Epoch Time: 3m 47s\n",
            "\tTrain Loss: 0.414 | Train Acc: 74.17%\n",
            "\t Val. Loss: 0.305 |  Val. Acc: 87.76%\n",
            "Epoch: 10 | Epoch Time: 3m 48s\n",
            "\tTrain Loss: 0.408 | Train Acc: 74.40%\n",
            "\t Val. Loss: 0.330 |  Val. Acc: 87.96%\n",
            "Epoch: 11 | Epoch Time: 3m 47s\n",
            "\tTrain Loss: 0.386 | Train Acc: 74.79%\n",
            "\t Val. Loss: 0.310 |  Val. Acc: 88.22%\n",
            "Epoch: 12 | Epoch Time: 3m 47s\n",
            "\tTrain Loss: 0.370 | Train Acc: 76.01%\n",
            "\t Val. Loss: 0.332 |  Val. Acc: 88.65%\n",
            "Epoch: 13 | Epoch Time: 3m 46s\n",
            "\tTrain Loss: 0.360 | Train Acc: 75.98%\n",
            "\t Val. Loss: 0.371 |  Val. Acc: 87.94%\n",
            "Epoch: 14 | Epoch Time: 3m 47s\n",
            "\tTrain Loss: 0.357 | Train Acc: 76.14%\n",
            "\t Val. Loss: 0.362 |  Val. Acc: 88.33%\n",
            "Epoch: 15 | Epoch Time: 3m 48s\n",
            "\tTrain Loss: 0.352 | Train Acc: 76.30%\n",
            "\t Val. Loss: 0.356 |  Val. Acc: 87.62%\n",
            "Epoch: 16 | Epoch Time: 3m 48s\n",
            "\tTrain Loss: 0.351 | Train Acc: 76.57%\n",
            "\t Val. Loss: 0.375 |  Val. Acc: 87.96%\n",
            "Epoch: 17 | Epoch Time: 3m 49s\n",
            "\tTrain Loss: 0.348 | Train Acc: 76.80%\n",
            "\t Val. Loss: 0.362 |  Val. Acc: 88.33%\n",
            "Epoch: 18 | Epoch Time: 3m 46s\n",
            "\tTrain Loss: 0.341 | Train Acc: 76.75%\n",
            "\t Val. Loss: 0.351 |  Val. Acc: 88.54%\n",
            "Epoch: 19 | Epoch Time: 3m 47s\n",
            "\tTrain Loss: 0.341 | Train Acc: 77.04%\n",
            "\t Val. Loss: 0.423 |  Val. Acc: 87.87%\n",
            "Epoch: 20 | Epoch Time: 3m 48s\n",
            "\tTrain Loss: 0.340 | Train Acc: 77.13%\n",
            "\t Val. Loss: 0.390 |  Val. Acc: 88.77%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6DjMsnZrgQp"
      },
      "source": [
        "...and get our new and vastly improved test accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpVhojKBrgQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8afb4b-faed-41dc-f54b-88b04b8d46f9"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.314 | Test Acc: 87.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PUDqscArgQs"
      },
      "source": [
        "## User Input\n",
        "\n",
        "We can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provided should also be movie reviews.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9xs5KiKrgQs"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2zeXfu2rgQu"
      },
      "source": [
        "An example negative review..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIsDUuA2rgQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d73495-a0bc-4f58-da7f-9da5e67df734"
      },
      "source": [
        "predict_sentiment(model, \"This film is terrible\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32540008425712585"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_DrhoWbvoNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97356e7-d431-4ceb-d562-dbe849427788"
      },
      "source": [
        "predict_sentiment(model, \"terrible is film This\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3562086522579193"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me1rxYcRrgQw"
      },
      "source": [
        "An example positive review..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adFtmxoNrgQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29a9bf9-ca30-4bc0-a3f1-09d29b58d192"
      },
      "source": [
        "predict_sentiment(model, \"This film is great\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8533573150634766"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auU1Wg5zvt2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4f3762-c38f-4b4a-b018-74d66d608099"
      },
      "source": [
        "predict_sentiment(model, \"great is film This\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8867491483688354"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5xT713PmfDl"
      },
      "source": [
        "def rev_sentence(sentence): \n",
        "  \n",
        "    # first split the string into words \n",
        "    words = sentence.split(' ')  \n",
        "  \n",
        "    # then reverse the split string list and join using space \n",
        "    reverse_sentence = ' '.join(reversed(words))  \n",
        "  \n",
        "    # finally return the joined string \n",
        "    return reverse_sentence   "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "5ZVKZm1_mhjY",
        "outputId": "cb098189-ed21-47c2-9bef-02fd3bffd8a2"
      },
      "source": [
        "sentence = \"Totally complete sci-fi comic book action movie with an excellent performance from Downey supported by a simple but solid script, superb effects and brilliant score.\"\n",
        "\n",
        "rev_txt = rev_sentence(sentence)\n",
        "\n",
        "rev_txt"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'score. brilliant and effects superb script, solid but simple a by supported Downey from performance excellent an with movie action book comic sci-fi complete Totally'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur7JxJqOlVsJ",
        "outputId": "a746224a-54e3-481e-c9c7-8cb18bc2d1ce"
      },
      "source": [
        "predict_sentiment(model, rev_txt)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9568549990653992"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtP5eUh0uSe2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}